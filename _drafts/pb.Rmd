---
title: "What is the cost of a progress bar in R?"
layout: default
published: false
category: Code
tags: [R, pbapply, progress bar]
disqus: petersolymos
promote: false
output: 
  pdf_document:
    keep_tex: true
---

```{r preamble,echo=FALSE}
knitr::opts_chunk$set(dev=c('png','pdf'),cache=TRUE,message=FALSE,warning=FALSE)
```

The [**pbapply**](http://cran.r-project.org/package=pbapply) R package
adds progress bar to vectorized functions, like `lapply`.
A [feature request](https://github.com/psolymos/pbapply/issues/9)
regarding progress bar for parallel functions has been sitting
at the development GitHub repo for a few months.
More recently, the author of the [**pbmcapply**](https://cran.r-project.org/web/packages/pbmcapply/index.html)
package dropped a note about his implementation of forking functionality
with progress bar for Unix/Linux computers, which got me thinking.
How should we add progress bar to snow type clusters? Which led to
more important questions: what is the real cost of the progress bar and how can we
reduce overhead on process times?

### Parallel workflow

Fist off, let's review how a
[**snow**](https://cran.r-project.org/web/packages/snow/index.html)
type cluster works. (This is actually implemented in the **parallel** package,
but I tend to refer to **snow** as that was the original package
where the **parallel** code was taken from, see
[here](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf).)

```{r snow}
parLapply
```

The `cl` argument refers to a cluster object that can be 
conveniently set up via the `makeCluster` function.
`X` is the 'problem' to be split among the *workers*
defined by the cluster object. `fun` is the function
that needs to be applied on elements of `X`.
Pretty much the same as the `lapply` function
except for the cluster argument.

Within the function, we see a nested expression that can be unpacked like this:

```{r unpack,eval=FALSE}
x <- splitList(X, length(cl))
z <- clusterApply(cl, x, fun = lapply, fun)
do.call(c, z, quote = TRUE)
```

The first line splits `X` so that it is now a list of lists. 
The list `x` is of length equal to the number of workers (i.e. `length(cl)`).
Each element in the list `x` is a part of the original `X` vector.
Let's see how `splitList` works. It is an unexported function
from the **parallel** package that calls `splitIndices`.
Suppose we have 10 'jobs' we want to submit to 2 workers
(note that these are just indices, not real values from `X`):

```{r split}
splitIndices(nx = 10, ncl = 2)
```

The first worker gets one half of the job, while the second gets the rest.
On other works, the full problem is split into `ncl` subsets,
and those are pushed to the workers in one batch.
That is what the next line with `clusterApply` does
(it uses `lapply` on each worker passing the `fun`ction etc.).
The output object `z` has all the results in the form of list of lists.
So the final `do.call` expression puts together the pieces in
a single list/vector that can be mapped to the input object `X`.

Here as example taking square of the numbers `11:20` (note: it is good practice 
to stop the child processes with `stopCluster` when done):

```{r snow_example}
library(parallel)
ncl <- 2
(cl <- makeCluster(ncl))
(X <- 11:20)
fun <- function(x) x^2
(x <- parallel:::splitList(X, length(cl))) # these are the real values
str(z <- clusterApply(cl, x, fun = lapply, fun))
str(do.call(c, z, quote = TRUE))
stopCluster(cl)
```

### Progress bar for sequential jobs

Here is a function that shows the basic principle
behind the functions in the functions in the **pbapply** package:

```{r pb}
pb_fun <- function(X, fun, ...) {
    X <- as.list(X) # X as list
    B <- length(X) # length of X
    fun <- match.fun(fun) # recognize the function
    out <- vector("list", B) # output list
    pb <- startpb(0, B) # set up progress bar
    for (i in seq_len(B)) { # start loop
        out[[i]] <- fun(X[[i]], ...) # apply function on X
        setpb(pb, i) # update progress bar
    } # end loop
    closepb(pb) # close progress bar on exit
    out # return output
}
pb_fun2 <- function(X, fun, ...) {
    X <- as.list(X) # X as list
    B <- length(X) # length of X
    fun <- match.fun(fun) # recognize the function
    out <- vector("list", B) # output list
    pb <- startpb(0, B) # set up progress bar
#    on.exit(closepb(pb)) # close progress bar on exit
    for (i in seq_len(B)) { # start loop
        out[[i]] <- fun(X[[i]], ...) # apply function on X
    } # end loop
    setpb(pb, i) # update progress bar
    closepb(pb) # close progress bar on exit
    out # return output
}
```

The example demonstrates that the progress bar is updated
at every iteration in the `for` loop, in this case it means 100
updates:

```{r sleep}
B <- 100
fun <- function(x) Sys.sleep(0.01)
pbo <- pboptions(type = "timer")
tmp <- pb_fun(1:B, fun)
tmp2 <- pb_fun2(1:B, fun)
pboptions(pbo)
```

If we want to put this updating process together with the
parallel evaluation, we need to split the problem
in a different way. Suppose we only send one job to
each worker at once. We can do that exactly `ceiling(B/ncl)` times.
The `splitpb` function takes care of the indices.
Using the previous example of 10 jobs and 2 workers, we get:

```{r splitpb}
splitpb(nx = 10, ncl = 2)
```

This gives us 10/2 = 5 occasions for updating the progress bar.
And it also means that we will increase communication overhead
for our parallel jobs 5-fold (from 1 to 5 batches).
The overhead increases linearly with problem size `B` as opposed
to being constant.

### Implementing progress bar for parallel jobs

Do some benchmarking (nout=NULL) -- table output

Problem: need to cap pb updates (examples: rjags, vegan)

Show results for nout: 500,200,100,50,25

Solution: use nout=100 as default as part of pboptions

Emphasize that this fixed overhead remains constant, old implementation depended on problem size.

```{r defs}
library(pbapply)
library(parallel)
library(pbmcapply)

pblapply_test <-
function (X, FUN, ..., cl = NULL, nout = NULL)
{
    FUN <- match.fun(FUN)
    if (!is.vector(X) || is.object(X))
        X <- as.list(X)
    ## catch single node requests and forking on Windows
    if (!is.null(cl)) {
        if (inherits(cl, "cluster") && length(cl) < 2L)
            cl <- NULL
        if (!inherits(cl, "cluster") && cl < 2)
            cl <- NULL
        if (!inherits(cl, "cluster") && .Platform$OS.type == "windows")
            cl <- NULL
    }
    ## sequential evaluation
    if (is.null(cl)) {
        if (!dopb())
            return(lapply(X, FUN, ...))
        Split <- splitpb(length(X), 1L, nout = nout)
        B <- length(Split)
        pb <- startpb(0, B)
        on.exit(closepb(pb), add = TRUE)
        rval <- vector("list", B)
        for (i in seq_len(B)) {
            rval[i] <- list(lapply(X[Split[[i]]], FUN, ...))
            setpb(pb, i)
        }
    ## parallel evaluation
    } else {
        ## snow type cluster
        if (inherits(cl, "cluster")) {
            if (!dopb())
                return(parallel::parLapply(cl, X, FUN, ...))
            ## define split here and use that for counter
            Split <- splitpb(length(X), length(cl), nout = nout)
            B <- length(Split)
            pb <- startpb(0, B)
            on.exit(closepb(pb), add = TRUE)
            rval <- vector("list", B)
            for (i in seq_len(B)) {
                rval[i] <- list(parallel::parLapply(cl, X[Split[[i]]], FUN, ...))
                setpb(pb, i)
            }
        ## multicore type forking
        } else {
            if (!dopb())
                return(parallel::mclapply(X, FUN, ..., mc.cores = as.integer(cl)))
            ## define split here and use that for counter
            Split <- splitpb(length(X), as.integer(cl), nout = nout)
            B <- length(Split)
            pb <- startpb(0, B)
            on.exit(closepb(pb), add = TRUE)
            rval <- vector("list", B)
            for (i in seq_len(B)) {
                rval[i] <- list(parallel::mclapply(X[Split[[i]]], FUN, ...,
                    mc.cores = as.integer(cl)))
                setpb(pb, i)
            }
        }
    }
    ## assemble output list
    rval <- do.call(c, rval, quote = TRUE)
    names(rval) <- names(X)
    rval
}

timer_fun <- function(X, FUN, type = "timer") {
    pbo <- pboptions(type = type)
    on.exit(pboptions(pbo))
    rbind(
        pb_NULL = system.time(pblapply_test(X, FUN, nout = NULL)),
        pb_100 = system.time(pblapply_test(X, FUN, nout = 100)),
        pb_50 = system.time(pblapply_test(X, FUN, nout = 50)),
        pb_25 = system.time(pblapply_test(X, FUN, nout = 25)),

        pb_cl_NULL = system.time(pblapply_test(X, FUN, cl = cl, nout = NULL)),
        pb_cl_100 = system.time(pblapply_test(X, FUN, cl = cl, nout = 100)),
        pb_cl_50 = system.time(pblapply_test(X, FUN, cl = cl, nout = 50)),
        pb_cl_25 = system.time(pblapply_test(X, FUN, cl = cl, nout = 25)),

        pb_mc_NULL = system.time(pblapply_test(X, FUN, cl = ncl, nout = NULL)),
        pb_mc_100 = system.time(pblapply_test(X, FUN, cl = ncl, nout = 100)),
        pb_mc_50 = system.time(pblapply_test(X, FUN, cl = ncl, nout = 50)),
        pb_mc_25 = system.time(pblapply_test(X, FUN, cl = ncl, nout = 25))
    )
}
```

```{r setup}
ncl <- 2
B <- 1000
fun <- function(x) Sys.sleep(0.01)
```

```{r calc,results="hide"}
cl <- makeCluster(ncl)
#clusterExport(cl, c("fun", "mod", "ndat", "bid"))
timer_out <- list(
    none = timer_fun(1:B, fun, type = "none"),
    txt = timer_fun(1:B, fun, type = "txt"),
    timer = timer_fun(1:B, fun, type = "timer"))
stopCluster(cl)
tpbmc <- system.time(pbmclapply(1:B, fun, mc.cores = ncl))
```

```{r pb_matplot,fig.height=4,fig.width=10}
npb <- c(B/ncl, 100, 50, 25)
tex <- 0.01 * B
tpb <- cbind(timer_out[[1]][1:4,3], 
    timer_out[[2]][1:4,3], 
    timer_out[[3]][1:4,3])
tcl <- cbind(timer_out[[1]][5:8,3], 
    timer_out[[2]][5:8,3], 
    timer_out[[3]][5:8,3])
tmc <- cbind(timer_out[[1]][9:12,3], 
    timer_out[[2]][9:12,3], 
    timer_out[[3]][9:12,3])
colnames(tpb) <- colnames(tcl) <- colnames(tmc) <- names(timer_out)

op <- par(mfrow = c(1, 3), las = 1, mar = c(5, 5, 2, 2))
matplot(c(B, 100, 50, 25), tpb, 
    type = "l", lty = 1, ylim = c(0, max(tpb, tcl, tmc)),
    xlab = "# pb updates", ylab = "proc time (sec)", main = "sequential")
abline(h = tex, lty = 2, col = 1)
matplot(c(B/ncl, 100, 50, 25), tcl, 
    type = "l", lty = 1, ylim = c(0, max(tpb, tcl, tmc)),
    xlab = "# pb updates", ylab = "proc time (sec)", main = "cluster")
abline(h = tex / ncl, lty = 2, col = 1)
matplot(c(B/ncl, 100, 50, 25), tmc, 
    type = "l", lty = 1, ylim = c(0, max(tpb, tcl, tmc)),
    xlab = "# pb updates", ylab = "proc time (sec)", main = "forking")
abline(h = tex / ncl, lty = 2, col = 1)
abline(h = tpbmc[3], lty = 2, col = 2)
legend("bottomleft", bty = "n", lty = c(1, 1, 1, 2, 2), col = c(1:3, 1, 2),
    legend = c(names(timer_out), "minimum", "pbmclapply"))
par(op)

tpb
tcl
tmc
```

`Sys.sleep` returns `NULL`.

```{r calc2,results="hide"}
fun <- function(x) {
    Sys.sleep(0.01)
    x^2
}

cl <- makeCluster(ncl)
#clusterExport(cl, c("fun", "mod", "ndat", "bid"))
timer_out <- list(
    none = timer_fun(1:B, fun, type = "none"),
    txt = timer_fun(1:B, fun, type = "txt"),
    timer = timer_fun(1:B, fun, type = "timer"))
stopCluster(cl)
tpbmc <- system.time(pbmclapply(1:B, fun, mc.cores = ncl))
```

```{r pb_matplot2,fig.height=4,fig.width=10}
npb <- c(B/ncl, 100, 50, 25)
tex <- 0.01 * B
tpb <- cbind(timer_out[[1]][1:4,3], 
    timer_out[[2]][1:4,3], 
    timer_out[[3]][1:4,3])
tcl <- cbind(timer_out[[1]][5:8,3], 
    timer_out[[2]][5:8,3], 
    timer_out[[3]][5:8,3])
tmc <- cbind(timer_out[[1]][9:12,3], 
    timer_out[[2]][9:12,3], 
    timer_out[[3]][9:12,3])
colnames(tpb) <- colnames(tcl) <- colnames(tmc) <- names(timer_out)

op <- par(mfrow = c(1, 3), las = 1, mar = c(5, 5, 2, 2))
matplot(c(B, 100, 50, 25), tpb, 
    type = "l", lty = 1, ylim = c(0, min(200, max(tpb, tcl, tmc))),
    xlab = "# pb updates", ylab = "proc time (sec)", main = "sequential")
abline(h = tex, lty = 2, col = 1)
matplot(c(B/ncl, 100, 50, 25), tcl, 
    type = "l", lty = 1, ylim = c(0, min(200, max(tpb, tcl, tmc))),
    xlab = "# pb updates", ylab = "proc time (sec)", main = "cluster")
abline(h = tex / ncl, lty = 2, col = 1)
matplot(c(B/ncl, 100, 50, 25), tmc, 
    type = "l", lty = 1, ylim = c(0, min(200, max(tpb, tcl, tmc))),
    xlab = "# pb updates", ylab = "proc time (sec)", main = "forking")
abline(h = tex / ncl, lty = 2, col = 1)
abline(h = tpbmc[3], lty = 2, col = 2)
legend("bottomleft", bty = "n", lty = c(1, 1, 1, 2, 2), col = c(1:3, 1, 2),
    legend = c(names(timer_out), "minimum", "pbmclapply"))
par(op)
tpb
tcl
tmc
```

### todo

* calculate system time, pb update time, communication time
* show that overhead is mostly due to increased communication and not pb update
* reason that parallel update have to be capped, possibly good for seq???
